{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyrl.environments import BetDiceRolling\n",
    "from pyrl.agents import Agent\n",
    "from pyrl.exp import Experiment\n",
    "\n",
    "from pyrl.tools import  ContinousState, DiscreteAction\n",
    "from pyrl.approximate.algorithms import deep_q_learning\n",
    "\n",
    "\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state: ContinousState, available_actions: List[DiscreteAction]) -> DiscreteAction:\n",
    "    return random.choice(available_actions)\n",
    "\n",
    "def policy(state: ContinousState, available_actions: List[DiscreteAction]) -> DiscreteAction:\n",
    "    return DiscreteAction(\"2\", 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = BetDiceRolling(10)\n",
    "\n",
    "exp = Experiment(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2184.50it/s]\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(random_policy)\n",
    "\n",
    "history = exp.explore(agent, 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q(nn.Module):\n",
    "\n",
    "    def __init__(self, sdim: int, adim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(sdim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, adim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Q(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = history[0][0]\n",
    "x = torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0013, 0.7402, 0.1127, 0.1458]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(x.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, new_states = [], [], [], []\n",
    "\n",
    "for transition in history:\n",
    "    states.append(transition[0])\n",
    "    actions.append(transition[1].number)\n",
    "    rewards.append(transition[2])\n",
    "    new_states.append(transition[3])\n",
    "\n",
    "states = torch.tensor(states)\n",
    "actions = torch.tensor(actions).reshape(-1, 1)\n",
    "rewards = torch.tensor(rewards).reshape(-1, 1)\n",
    "new_states = torch.tensor(new_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
